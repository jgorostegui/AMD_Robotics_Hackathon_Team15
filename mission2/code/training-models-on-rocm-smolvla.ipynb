{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQUk3Y0WwYZ4"
   },
   "source": [
    "# Train Models Using LeRobot on MI300x\n",
    "\n",
    "This guide walks you through setting up environment for training imitation learning policies using LeRobot library on a DigitalOcean (DO) instance equipped with AMD MI300x GPUs and ROCm.\n",
    "\n",
    "## ⚙️ Requirements\n",
    "- A Hugging Face dataset repo ID containing your training data (`--dataset.repo_id=${HF_USER}/${DATASET_NAME}`).\n",
    "  If you don’t have an access token yet, you can sign up for Hugging Face [here](https://huggingface.co/join). After signing up, create an access token by visiting [here](https://huggingface.co/settings/tokens).\n",
    "- A wandb account to enable training visualization and upload your training evidence to our github.\n",
    "  You can sign up for Wandb [here](https://wandb.ai/signup) and visit [here](https://wandb.ai/authorize) to create a token.\n",
    "- Access to DO instance AMD Mi300x GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJyX0CnwA5m"
   },
   "source": [
    "## Verify ROCm and GPU availability\n",
    "This cell uses `pytorch` to check AMD GPU Info. The expected ouput is \n",
    "```\n",
    "CUDA compatible device availability: True\n",
    "device name [0]: AMD Instinct MI300X VF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA compatible device availability: True\n",
      "device name [0]: AMD Instinct MI300X VF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'CUDA compatible device availability:',torch.cuda.is_available())\n",
    "print(f'device name [0]:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJyX0CnwA5m"
   },
   "source": [
    "## Install FFmpeg 7.x\n",
    "This cell uses `apt` to install ffmpeg 7.x for LeRobot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlKjL1X5t_zM",
    "outputId": "3b375aa1-19ed-4811-ff76-0afd5b762992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: 'Types: deb\n",
      "URIs: https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu/\n",
      "Suites: noble\n",
      "Components: main\n",
      "'\n",
      "Description:\n",
      "unofficial build for FFmpeg 7 for Ubuntu 22.04 | 24.04, backport from Debian's deb.multimedia.org repository\n",
      "\n",
      "If the packages here are helpful, you may buy me a coffee:\n",
      "\n",
      "         https://ko-fi.com/ubuntuhandbook1\n",
      "More info: https://launchpad.net/~ubuntuhandbook1/+archive/ubuntu/ffmpeg7\n",
      "Adding repository.\n",
      "Found existing deb entry in /etc/apt/sources.list.d/ubuntuhandbook1-ubuntu-ffmpeg7-noble.sources\n",
      "Hit:1 https://repo.radeon.com/amdgpu/30.10/ubuntu jammy InRelease\n",
      "Hit:2 https://repo.radeon.com/rocm/apt/7.0 jammy InRelease                     \n",
      "Hit:3 https://repo.radeon.com/graphics/7.0/ubuntu jammy InRelease              \n",
      "Get:4 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]      \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu noble InRelease                         \n",
      "Get:6 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]        \n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease   \n",
      "Hit:8 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble InRelease\n",
      "Get:9 http://security.ubuntu.com/ubuntu noble-security/main amd64 Components [25.4 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Components [91.3 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Components [156 B]\n",
      "Get:12 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Components [158 B]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [2118 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Components [235 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1943 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Components [520 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Components [160 B]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Components [888 B]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Components [7877 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Components [11.9 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu noble-backports/restricted amd64 Components [160 B]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu noble-backports/multiverse amd64 Components [162 B]\n",
      "Fetched 5332 kB in 2s (2895 kB/s)                                 \n",
      "Reading package lists... Done\n",
      "Hit:1 https://repo.radeon.com/amdgpu/30.10/ubuntu jammy InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu noble InRelease                         \u001b[0m\n",
      "Hit:3 http://security.ubuntu.com/ubuntu noble-security InRelease               \u001b[0m\n",
      "Hit:4 https://repo.radeon.com/rocm/apt/7.0 jammy InRelease                     \u001b[0m\u001b[33m\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu noble-updates InRelease                 \u001b[0m\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu noble-backports InRelease               \u001b[0m\n",
      "Hit:7 https://repo.radeon.com/graphics/7.0/ubuntu jammy InRelease              \u001b[0m\u001b[33m\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease   \n",
      "Hit:9 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "93 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:7.1.1-0build1~ubuntu2404).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 93 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!add-apt-repository ppa:ubuntuhandbook1/ffmpeg7 -y # install PPA which contains ffmpeg 7.x\n",
    "!apt update && apt install ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxCc3CARwUjN"
   },
   "source": [
    "## Install LeRobot v0.4.1\n",
    "This cell clones the `lerobot` repository from Hugging Face, and installs the package in editable mode. Extra Features: To install additional dependencies for training SmolVLA or Pi models, refer to the [LeRobot offical page](https://huggingface.co/docs/lerobot/index). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgLu7QT5tUik",
    "outputId": "e46913b8-1977-48a5-a851-d8c69602419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'lerobot' already exists and is not an empty directory.\n",
      "warning: refname 'v0.4.1' is ambiguous.\n",
      "fatal: a branch named 'v0.4.1' already exists\n",
      "Obtaining file:///workspace/lerobot\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets<4.2.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.1)\n",
      "Requirement already satisfied: diffusers<0.36.0,>=0.27.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.35.2)\n",
      "Requirement already satisfied: huggingface-hub<0.36.0,>=0.34.2 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.35.3)\n",
      "Requirement already satisfied: accelerate<2.0.0,>=1.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.12.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (80.9.0)\n",
      "Requirement already satisfied: cmake<4.2.0,>=3.29.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.3)\n",
      "Requirement already satisfied: einops<0.9.0,>=0.8.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.8.1)\n",
      "Requirement already satisfied: opencv-python-headless<4.13.0,>=4.9.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.12.0.88)\n",
      "Requirement already satisfied: av<16.0.0,>=15.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (15.1.0)\n",
      "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.0.0)\n",
      "Requirement already satisfied: packaging<26.0,>=24.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (25.0)\n",
      "Requirement already satisfied: pynput<1.9.0,>=1.7.7 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.8.1)\n",
      "Requirement already satisfied: pyserial<4.0,>=3.5 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: wandb<0.22.0,>=0.20.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.21.4)\n",
      "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (2.7.1+rocm7.0.0.lw.git698b58a9)\n",
      "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.5)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.22.1+rocm7.0.0.git59a3e1f9)\n",
      "Requirement already satisfied: draccus==0.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.10.0)\n",
      "Requirement already satisfied: gymnasium<2.0.0,>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.2.2)\n",
      "Requirement already satisfied: rerun-sdk<0.27.0,>=0.24.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.26.2)\n",
      "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (8.6.1)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (2.37.2)\n",
      "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.2.0)\n",
      "Requirement already satisfied: mergedeep~=1.3 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.3.4)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (6.0.2)\n",
      "Requirement already satisfied: pyyaml-include~=1.4 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.4.1)\n",
      "Requirement already satisfied: toml~=0.10 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.10.2)\n",
      "Requirement already satisfied: typing-inspect~=0.9.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (2.2.6)\n",
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (7.1.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.9.0)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in /opt/venv/lib/python3.12/site-packages (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.1) (5.5.0)\n",
      "Requirement already satisfied: importlib_metadata in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (8.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (2025.11.3)\n",
      "Requirement already satisfied: Pillow in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (11.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.13.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (0.0.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (1.2.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.1.9)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (3.0.52)\n",
      "Requirement already satisfied: imageio-ffmpeg in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (0.6.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/venv/lib/python3.12/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.1) (25.4.0)\n",
      "Requirement already satisfied: wcwidth in /opt/venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.2.14)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.17.0)\n",
      "Requirement already satisfied: evdev>=1.3 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.9.2)\n",
      "Requirement already satisfied: python-xlib>=0.17 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (0.33)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.1.6)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.3.1+rocm7.0.0.git9c7bc0a3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.3.1+rocm7.0.0.git9c7bc0a3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.1) (1.1.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (6.33.2)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.47.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/venv/lib/python3.12/site-packages (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Building wheels for collected packages: lerobot\n",
      "  Building editable for lerobot (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lerobot: filename=lerobot-0.4.1-0.editable-py3-none-any.whl size=15631 sha256=fe07bdfec251b1c298101f4fdb4ffd816cdf01d9632d6fd64e3c53426f253d00\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tev1ikz4/wheels/05/0a/0d/80a4c08845345c44fe1e5f70929884983b90d85f46a77f7601\n",
      "Successfully built lerobot\n",
      "Installing collected packages: lerobot\n",
      "  Attempting uninstall: lerobot\n",
      "    Found existing installation: lerobot 0.4.1\n",
      "    Uninstalling lerobot-0.4.1:\n",
      "      Successfully uninstalled lerobot-0.4.1\n",
      "Successfully installed lerobot-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/lerobot.git\n",
    "!cd lerobot && git checkout -b v0.4.1 v0.4.1 # let’s synchronize using this version\n",
    "!cd lerobot && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/lerobot\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets<4.2.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.1)\n",
      "Requirement already satisfied: diffusers<0.36.0,>=0.27.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.35.2)\n",
      "Requirement already satisfied: huggingface-hub<0.36.0,>=0.34.2 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.35.3)\n",
      "Requirement already satisfied: accelerate<2.0.0,>=1.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.12.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (80.9.0)\n",
      "Requirement already satisfied: cmake<4.2.0,>=3.29.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.3)\n",
      "Requirement already satisfied: einops<0.9.0,>=0.8.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.8.1)\n",
      "Requirement already satisfied: opencv-python-headless<4.13.0,>=4.9.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.12.0.88)\n",
      "Requirement already satisfied: av<16.0.0,>=15.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (15.1.0)\n",
      "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.0.0)\n",
      "Requirement already satisfied: packaging<26.0,>=24.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (25.0)\n",
      "Requirement already satisfied: pynput<1.9.0,>=1.7.7 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.8.1)\n",
      "Requirement already satisfied: pyserial<4.0,>=3.5 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: wandb<0.22.0,>=0.20.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.21.4)\n",
      "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (2.7.1+rocm7.0.0.lw.git698b58a9)\n",
      "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.5)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.22.1+rocm7.0.0.git59a3e1f9)\n",
      "Requirement already satisfied: draccus==0.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.10.0)\n",
      "Requirement already satisfied: gymnasium<2.0.0,>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.2.2)\n",
      "Requirement already satisfied: rerun-sdk<0.27.0,>=0.24.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.26.2)\n",
      "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (8.6.1)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (2.37.2)\n",
      "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.2.0)\n",
      "Collecting num2words<0.6.0,>=0.5.14 (from lerobot==0.4.1)\n",
      "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: safetensors<1.0.0,>=0.4.3 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: mergedeep~=1.3 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.3.4)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (6.0.2)\n",
      "Requirement already satisfied: pyyaml-include~=1.4 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.4.1)\n",
      "Requirement already satisfied: toml~=0.10 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.10.2)\n",
      "Requirement already satisfied: typing-inspect~=0.9.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (2.2.6)\n",
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (7.1.3)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.9.0)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in /opt/venv/lib/python3.12/site-packages (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.1) (5.5.0)\n",
      "Requirement already satisfied: importlib_metadata in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (8.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (2025.11.3)\n",
      "Requirement already satisfied: Pillow in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (11.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.13.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (0.0.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (1.2.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.1.9)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (3.0.52)\n",
      "Requirement already satisfied: imageio-ffmpeg in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (0.6.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/venv/lib/python3.12/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.1) (25.4.0)\n",
      "Collecting docopt>=0.6.2 (from num2words<0.6.0,>=0.5.14->lerobot==0.4.1)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /opt/venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.2.14)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.17.0)\n",
      "Requirement already satisfied: evdev>=1.3 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.9.2)\n",
      "Requirement already satisfied: python-xlib>=0.17 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (0.33)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.1.6)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.3.1+rocm7.0.0.git9c7bc0a3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.3.1+rocm7.0.0.git9c7bc0a3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.1) (1.1.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (6.33.2)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.47.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/venv/lib/python3.12/site-packages (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.0.2)\n",
      "Collecting transformers<5.0.0,>=4.53.0 (from lerobot==0.4.1)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.53.0->lerobot==0.4.1)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lerobot, docopt\n",
      "  Building editable for lerobot (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lerobot: filename=lerobot-0.4.1-0.editable-py3-none-any.whl size=15631 sha256=bf86f144086ae16e886ec00f028940b0bf91713d1ffab8d619d3401d24868caa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dgy46g94/wheels/05/0a/0d/80a4c08845345c44fe1e5f70929884983b90d85f46a77f7601\n",
      "  Building wheel for docopt (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=1d701d332cfa5b18cddaf2a2e8c1366e910c0f98b0c66536801714ec98c5a377\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built lerobot docopt\n",
      "Installing collected packages: docopt, num2words, tokenizers, transformers, lerobot\n",
      "\u001b[2K  Attempting uninstall: lerobot━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: lerobot 0.4.1m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling lerobot-0.4.1:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled lerobot-0.4.1\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [lerobot]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [lerobot]\u001b[0m [lerobot]\n",
      "\u001b[1A\u001b[2KSuccessfully installed docopt-0.6.2 lerobot-0.4.1 num2words-0.5.14 tokenizers-0.22.1 transformers-4.57.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd lerobot && pip install -e \".[smolvla]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8Sn2wG4wldo"
   },
   "source": [
    "## Weights & Biases login\n",
    "This cell install and log into Weights & Biases (wandb) to enable experiment tracking and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PolVM_movEvp",
    "outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/venv/lib/python3.12/site-packages (0.21.4)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.12/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (6.33.2)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in /opt/venv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/venv/lib/python3.12/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlamperez10\u001b[0m (\u001b[33mjlamperez10-particular\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login(key=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login into Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PolVM_movEvp",
    "outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzTo4mNwxaC"
   },
   "source": [
    "## Start Training Models with LeRobot\n",
    "\n",
    "This cell uses the lerobot-train CLI from the lerobot library to train a robot control policy.  \n",
    "\n",
    "Make sure to adjust the following arguments to your setup:\n",
    "\n",
    "1. `--dataset.repo_id=YOUR_HF_USERNAME/YOUR_DATASET`:  \n",
    "   Replace this with the Hugging Face Hub repo ID where your dataset is stored, e.g., `lerobot/svla_so100_pickplace`.\n",
    "\n",
    "2. `--policy.type=act`:  \n",
    "   Specifies the policy configuration to use. `act` refers to [configuration_act.py](../lerobot/common/policies/act/configuration_act.py), which will automatically adapt to your dataset’s setup (e.g., number of motors and cameras).\n",
    "\n",
    "3. `--output_dir=outputs/train/...`:  \n",
    "   Directory where training logs and model checkpoints will be saved.\n",
    "\n",
    "4. `--job_name=...`:  \n",
    "   A name for this training job, used for logging and Weights & Biases.The name typically includes the model type (e.g., act, smolvla), the dataset name, and additional descriptive tags.\n",
    "\n",
    "5. `--policy.device=cuda`:  \n",
    "   Use `cuda` if training on an AMD or NVIDIA GPU. \n",
    "\n",
    "6. `--wandb.enable=true`:  \n",
    "   Enables Weights & Biases for visualizing training progress. You must be logged in via `wandb login` before running this.\n",
    "\n",
    "7. `--policy.push_to_hub=`:\n",
    "\n",
    "   Enables automatic uploading of the trained policy to the Hugging Face Hub. You must specify `--policy.repo_id` (e.g., ${HF_USER}/{REPO_NAME}) if it is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m[INFO]\u001b[0m Installing Hugging Face CLI...\n",
      "\u001b[0;34m[INFO]\u001b[0m OS: linux\n",
      "\u001b[0;34m[INFO]\u001b[0m Force reinstall: false\n",
      "\u001b[0;34m[INFO]\u001b[0m Install dir: /root/.hf-cli\n",
      "\u001b[0;34m[INFO]\u001b[0m Bin dir: /root/.local/bin\n",
      "\u001b[0;34m[INFO]\u001b[0m Requested version: latest\n",
      "\u001b[0;34m[INFO]\u001b[0m Skip PATH update: false\n",
      "\u001b[0;34m[INFO]\u001b[0m Using Python: Python 3.12.3\n",
      "\u001b[0;34m[INFO]\u001b[0m Creating directories...\n",
      "\u001b[0;34m[INFO]\u001b[0m Creating virtual environment...\n",
      "\u001b[0;34m[INFO]\u001b[0m Upgrading pip...\n",
      "Requirement already satisfied: pip in /root/.hf-cli/venv/lib/python3.12/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-25.3\n",
      "\u001b[0;34m[INFO]\u001b[0m Installing The Hugging Face CLI (latest)...\n",
      "\u001b[0;34m[INFO]\u001b[0m Installation output suppressed; set HF_CLI_VERBOSE_PIP=1 for full logs\n",
      "\u001b[0;34m[INFO]\u001b[0m Linking hf CLI into /root/.local/bin...\n",
      "\u001b[0;34m[INFO]\u001b[0m hf available at /root/.local/bin/hf (symlink to venv)\n",
      "\u001b[0;34m[INFO]\u001b[0m Run without touching PATH: env PATH=\"/root/.local/bin:$PATH\" hf --help\n",
      "\u001b[0;34m[INFO]\u001b[0m /root/.local/bin is not in your PATH\n",
      "\u001b[0;32m[SUCCESS]\u001b[0m Added /root/.local/bin to PATH via /root/.bashrc\n",
      "\u001b[0;34m[INFO]\u001b[0m Apply it now with: source /root/.bashrc\n",
      "\u001b[0;34m[INFO]\u001b[0m Verifying installation...\n",
      "\u001b[0;32m[SUCCESS]\u001b[0m Hugging Face CLI installed successfully!\n",
      "\u001b[0;34m[INFO]\u001b[0m CLI location: /root/.local/bin/hf\n",
      "\u001b[0;34m[INFO]\u001b[0m Installation directory: /root/.hf-cli\n",
      "\u001b[0;34m[INFO]\u001b[0m Current version: 1.2.3\n",
      "\u001b[0;34m[INFO]\u001b[0m \n",
      "\u001b[0;34m[INFO]\u001b[0m To uninstall the Hugging Face CLI, run:\n",
      "\u001b[0;34m[INFO]\u001b[0m   rm -rf /root/.hf-cli\n",
      "\u001b[0;34m[INFO]\u001b[0m   rm -f /root/.local/bin/hf\n",
      "\u001b[0;34m[INFO]\u001b[0m \n",
      "\u001b[0;34m[INFO]\u001b[0m   (shell) Undo PATH entry: sed -i.bak '/Added by Hugging Face CLI installer/d' /root/.bashrc && rm -f /root/.bashrc.bak\n",
      "\u001b[0;32m[SUCCESS]\u001b[0m hf CLI ready!\n",
      "\u001b[0;34m[INFO]\u001b[0m Binary: /root/.local/bin/hf\n",
      "\u001b[0;34m[INFO]\u001b[0m Virtualenv: /root/.hf-cli\n",
      "\u001b[0;34m[INFO]\u001b[0m CLI version: latest\n",
      "\u001b[0;34m[INFO]\u001b[0m Try it now: env PATH=\"/root/.local/bin:$PATH\" hf --help\n",
      "\u001b[0;34m[INFO]\u001b[0m Examples:\n",
      "\u001b[0;34m[INFO]\u001b[0m   hf auth login\n",
      "\u001b[0;34m[INFO]\u001b[0m   hf download deepseek-ai/DeepSeek-R1\n",
      "\u001b[0;34m[INFO]\u001b[0m   hf jobs run python:3.12 python -c 'print(\"Hello from HF CLI!\")'\n",
      "\u001b[0;34m[INFO]\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "!curl -LsSf https://hf.co/cli/install.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|████████████████████| 10/10 [00:00<00:00, 109798.53it/s]\n",
      "/root/.cache/huggingface/hub/models--lerobot--smolvla_base/snapshots/4d2f2b37fa245361ef1efe6d91ce96b8bd4af511\n"
     ]
    }
   ],
   "source": [
    "!hf download lerobot/smolvla_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli whoami' is deprecated. Use 'hf auth whoami' instead.\u001b[0m\n",
      "jlamperez\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ufss6US6xbpi",
    "outputId": "cfd494c7-8689-419d-bb17-7f02a67b26d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-14 03:52:00 ot_train.py:163 {'batch_size': 64,\n",
      " 'checkpoint_path': None,\n",
      " 'dataset': {'episodes': None,\n",
      "             'image_transforms': {'enable': False,\n",
      "                                  'max_num_transforms': 3,\n",
      "                                  'random_order': False,\n",
      "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
      "                                                                            5.0],\n",
      "                                                                'translate': [0.05,\n",
      "                                                                              0.05]},\n",
      "                                                     'type': 'RandomAffine',\n",
      "                                                     'weight': 1.0},\n",
      "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
      "                                                                                   1.2]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
      "                                                                               1.2]},\n",
      "                                                       'type': 'ColorJitter',\n",
      "                                                       'weight': 1.0},\n",
      "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
      "                                                                     0.05]},\n",
      "                                                  'type': 'ColorJitter',\n",
      "                                                  'weight': 1.0},\n",
      "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
      "                                                                                   1.5]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
      "                                                                                 1.5]},\n",
      "                                                        'type': 'SharpnessJitter',\n",
      "                                                        'weight': 1.0}}},\n",
      "             'repo_id': 'jlamperez/mission2_smolvla_multitask_v2_120ep',\n",
      "             'revision': None,\n",
      "             'root': None,\n",
      "             'streaming': False,\n",
      "             'use_imagenet_stats': True,\n",
      "             'video_backend': 'torchcodec'},\n",
      " 'env': None,\n",
      " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
      " 'eval_freq': 20000,\n",
      " 'job_name': 'my_smolvla_training_30ksteps_120ep',\n",
      " 'log_freq': 200,\n",
      " 'num_workers': 4,\n",
      " 'optimizer': {'betas': [0.9, 0.95],\n",
      "               'eps': 1e-08,\n",
      "               'grad_clip_norm': 10.0,\n",
      "               'lr': 0.0001,\n",
      "               'type': 'adamw',\n",
      "               'weight_decay': 1e-10},\n",
      " 'output_dir': 'outputs/train/mission2_smolvla_multitask_30ksteps_120ep',\n",
      " 'policy': {'adapt_to_pi_aloha': False,\n",
      "            'add_image_special_tokens': False,\n",
      "            'attention_mode': 'cross_attn',\n",
      "            'chunk_size': 50,\n",
      "            'device': 'cuda',\n",
      "            'empty_cameras': 0,\n",
      "            'expert_width_multiplier': 0.75,\n",
      "            'freeze_vision_encoder': True,\n",
      "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.images.camera2': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.images.camera3': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.state': {'shape': [6],\n",
      "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
      "            'license': None,\n",
      "            'load_vlm_weights': True,\n",
      "            'max_action_dim': 32,\n",
      "            'max_period': 4.0,\n",
      "            'max_state_dim': 32,\n",
      "            'min_period': 0.004,\n",
      "            'n_action_steps': 50,\n",
      "            'n_obs_steps': 1,\n",
      "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
      "            'num_expert_layers': 0,\n",
      "            'num_steps': 10,\n",
      "            'num_vlm_layers': 16,\n",
      "            'optimizer_betas': [0.9, 0.95],\n",
      "            'optimizer_eps': 1e-08,\n",
      "            'optimizer_grad_clip_norm': 10.0,\n",
      "            'optimizer_lr': 0.0001,\n",
      "            'optimizer_weight_decay': 1e-10,\n",
      "            'output_features': {'action': {'shape': [6],\n",
      "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
      "            'pad_language_to': 'max_length',\n",
      "            'prefix_length': 0,\n",
      "            'pretrained_path': 'lerobot/smolvla_base',\n",
      "            'private': None,\n",
      "            'push_to_hub': True,\n",
      "            'repo_id': 'jlamperez/mission2_smolvla_multitask_policy_30ksteps_120ep',\n",
      "            'resize_imgs_with_padding': [512, 512],\n",
      "            'scheduler_decay_lr': 2.5e-06,\n",
      "            'scheduler_decay_steps': 30000,\n",
      "            'scheduler_warmup_steps': 1000,\n",
      "            'self_attn_every_n_layers': 2,\n",
      "            'tags': None,\n",
      "            'tokenizer_max_length': 48,\n",
      "            'train_expert_only': True,\n",
      "            'train_state_proj': True,\n",
      "            'type': 'smolvla',\n",
      "            'use_amp': True,\n",
      "            'use_cache': True,\n",
      "            'use_delta_joint_actions_aloha': False,\n",
      "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
      " 'rename_map': {},\n",
      " 'resume': False,\n",
      " 'save_checkpoint': True,\n",
      " 'save_freq': 5000,\n",
      " 'scheduler': {'decay_lr': 2.5e-06,\n",
      "               'num_decay_steps': 30000,\n",
      "               'num_warmup_steps': 1000,\n",
      "               'peak_lr': 0.0001,\n",
      "               'type': 'cosine_decay_with_warmup'},\n",
      " 'seed': 1000,\n",
      " 'steps': 30000,\n",
      " 'use_policy_training_preset': True,\n",
      " 'wandb': {'disable_artifact': False,\n",
      "           'enable': True,\n",
      "           'entity': None,\n",
      "           'mode': None,\n",
      "           'notes': None,\n",
      "           'project': 'lerobot',\n",
      "           'run_id': None}}\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "INFO 2025-12-14 03:52:02 db_utils.py:102 \u001b[1m\u001b[34mLogs will be synced with wandb.\u001b[0m\n",
      "INFO 2025-12-14 03:52:02 db_utils.py:103 Track this run --> \u001b[1m\u001b[33mhttps://wandb.ai/jlamperez10-particular/lerobot/runs/6ima45d9\u001b[0m\n",
      "INFO 2025-12-14 03:52:02 ot_train.py:183 Creating dataset\n",
      "Fetching 33 files:   0%|                                 | 0/33 [00:00<?, ?it/s]\n",
      "meta/episodes/chunk-000/file-000.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-001.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-006.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-005.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-007.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-002.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-003.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-004.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-006.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 175k\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-007.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 196k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-005.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 168k\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-002.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 189k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-005.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 160k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-002.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 181k\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-001.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 130k\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-004.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 180k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-004.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 180k\u001b[A\n",
      "meta/episodes/chunk-000/file-003.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 174k\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-000.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 121k\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-001.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 123k\n",
      "Fetching 33 files:   3%|▊                        | 1/33 [00:00<00:24,  1.31it/s]\n",
      "meta/episodes/chunk-000/file-009.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-011.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-010.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-008.parquet: 100%|█| 66.2k/66.2k [00:00<00:00, 539k\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-014.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-008.parquet: 100%|█| 66.2k/66.2k [00:00<00:00, 460k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-015.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-009.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 578k\u001b[A\n",
      "Fetching 33 files:  30%|███████▎                | 10/33 [00:00<00:01, 14.36it/s]\n",
      "\n",
      "meta/episodes/chunk-000/file-011.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 612k\u001b[A\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-013.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 717k\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-010.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 612k\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-014.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 532k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-010.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 453k\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-015.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 472k\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-015.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 329k\u001b[A\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-016.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 484k\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-020.parquet:   0%|     | 0.00/66.2k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-019.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-018.parquet:   0%|     | 0.00/66.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-022.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-023.parquet:   0%|     | 0.00/66.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-017.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 615k\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-019.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 586k\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-018.parquet: 100%|█| 66.4k/66.4k [00:00<00:00, 541k\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-024.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-012.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-025.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-021.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-012.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 657k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 33 files:  42%|██████████▏             | 14/33 [00:01<00:01, 11.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-022.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 279k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-022.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 278k\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-023.parquet: 100%|█| 66.2k/66.2k [00:00<00:00, 289k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-024.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 523k\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-025.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 597k\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-025.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 582k\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-020.parquet: 100%|█| 66.2k/66.2k [00:00<00:00, 194k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-021.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 522k\n",
      "\n",
      "meta/episodes/chunk-000/file-028.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "meta/episodes/chunk-000/file-027.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-029.parquet:   0%|     | 0.00/66.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "info.json: 3.87kB [00:00, 1.48MB/s]A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/tasks.parquet:   0%|                           | 0.00/2.38k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "stats.json: 16.1kB [00:00, 58.4MB/s]A\u001b[A\u001b[A\u001b[A\n",
      "meta/episodes/chunk-000/file-028.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 709k\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-027.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 561k\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-029.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 568k\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meta/episodes/chunk-000/file-026.parquet: 100%|█| 66.5k/66.5k [00:00<00:00, 216k\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 33 files:  82%|███████████████████▋    | 27/33 [00:01<00:00, 21.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "meta/tasks.parquet: 100%|██████████████████| 2.38k/2.38k [00:00<00:00, 16.3kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 33 files: 100%|████████████████████████| 33/33 [00:01<00:00, 19.53it/s]\n",
      "Fetching 154 files:   0%|                               | 0/154 [00:00<?, ?it/s]\n",
      "data/chunk-000/file-000.parquet:   0%|               | 0.00/103k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      ".gitattributes: 2.46kB [00:00, 456kB/s][A\n",
      "data/chunk-000/file-000.parquet: 100%|███████| 103k/103k [00:00<00:00, 1.16MB/s]\n",
      "\n",
      "data/chunk-000/file-001.parquet:   0%|               | 0.00/105k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "data/chunk-000/file-003.parquet:   0%|              | 0.00/99.9k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-004.parquet:   0%|               | 0.00/102k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-007.parquet:   0%|              | 0.00/99.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-008.parquet:   0%|              | 0.00/99.9k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-005.parquet:   0%|               | 0.00/106k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-006.parquet:   0%|               | 0.00/103k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-001.parquet: 100%|████████| 105k/105k [00:00<00:00, 837kB/s]\u001b[A\n",
      "data/chunk-000/file-007.parquet: 100%|█████| 99.6k/99.6k [00:00<00:00, 1.14MB/s]\n",
      "data/chunk-000/file-008.parquet: 100%|█████| 99.9k/99.9k [00:00<00:00, 1.13MB/s]\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-004.parquet: 100%|████████| 102k/102k [00:00<00:00, 981kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "data/chunk-000/file-004.parquet: 100%|████████| 102k/102k [00:00<00:00, 938kB/s]\u001b[A\u001b[A\n",
      "data/chunk-000/file-003.parquet: 100%|██████| 99.9k/99.9k [00:00<00:00, 810kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-005.parquet: 100%|████████| 106k/106k [00:00<00:00, 900kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-005.parquet: 100%|████████| 106k/106k [00:00<00:00, 860kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-006.parquet: 100%|████████| 103k/103k [00:00<00:00, 816kB/s]\n",
      "\n",
      "data/chunk-000/file-009.parquet:   0%|              | 0.00/99.4k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "data/chunk-000/file-012.parquet:   0%|               | 0.00/100k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-010.parquet:   0%|              | 0.00/99.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-013.parquet:   0%|              | 0.00/97.8k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-014.parquet:   0%|              | 0.00/99.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-011.parquet:   0%|              | 0.00/95.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-009.parquet: 100%|██████| 99.4k/99.4k [00:00<00:00, 869kB/s]\u001b[A\n",
      "\n",
      "data/chunk-000/file-010.parquet: 100%|█████| 99.2k/99.2k [00:00<00:00, 1.00MB/s]\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-012.parquet: 100%|████████| 100k/100k [00:00<00:00, 898kB/s]\u001b[A\u001b[A\n",
      "data/chunk-000/file-014.parquet: 100%|█████| 99.7k/99.7k [00:00<00:00, 1.16MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-013.parquet: 100%|██████| 97.8k/97.8k [00:00<00:00, 837kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-011.parquet: 100%|██████| 95.6k/95.6k [00:00<00:00, 878kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "data/chunk-000/file-011.parquet: 100%|██████| 95.6k/95.6k [00:00<00:00, 818kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-017.parquet:   0%|              | 0.00/98.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-018.parquet:   0%|              | 0.00/96.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-015.parquet: 100%|████████| 103k/103k [00:00<00:00, 883kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-015.parquet: 100%|████████| 103k/103k [00:00<00:00, 831kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "data/chunk-000/file-016.parquet: 100%|█████| 99.5k/99.5k [00:00<00:00, 1.22MB/s]\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-021.parquet:   0%|               | 0.00/102k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-018.parquet: 100%|█████| 96.6k/96.6k [00:00<00:00, 1.11MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-022.parquet:   0%|              | 0.00/98.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-017.parquet: 100%|██████| 98.3k/98.3k [00:00<00:00, 806kB/s]\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-002.parquet: 100%|███████| 101k/101k [00:00<00:00, 1.00MB/s]\n",
      "data/chunk-000/file-019.parquet: 100%|█████| 98.0k/98.0k [00:00<00:00, 1.06MB/s]\n",
      "Fetching 154 files:   3%|▌                      | 4/154 [00:00<00:36,  4.16it/s]\n",
      "data/chunk-000/file-023.parquet:   0%|               | 0.00/100k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-024.parquet:   0%|               | 0.00/101k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "data/chunk-000/file-021.parquet: 100%|████████| 102k/102k [00:00<00:00, 918kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-025.parquet:   0%|               | 0.00/108k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-027.parquet:   0%|              | 0.00/96.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-020.parquet: 100%|██████| 99.3k/99.3k [00:00<00:00, 699kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-023.parquet: 100%|███████| 100k/100k [00:00<00:00, 1.01MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-022.parquet: 100%|██████| 98.0k/98.0k [00:00<00:00, 664kB/s]\n",
      "data/chunk-000/file-020.parquet: 100%|██████| 99.3k/99.3k [00:00<00:00, 591kB/s]\n",
      "Fetching 154 files:  14%|███▏                  | 22/154 [00:00<00:04, 32.83it/s]\n",
      "data/chunk-000/file-026.parquet:   0%|               | 0.00/102k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-028.parquet:   0%|               | 0.00/103k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-024.parquet: 100%|████████| 101k/101k [00:00<00:00, 813kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "data/chunk-000/file-024.parquet: 100%|████████| 101k/101k [00:00<00:00, 781kB/s]\u001b[A\u001b[A\n",
      "data/chunk-000/file-025.parquet: 100%|████████| 108k/108k [00:00<00:00, 879kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-027.parquet: 100%|██████| 96.0k/96.0k [00:00<00:00, 865kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "data/chunk-000/file-029.parquet:   0%|               | 0.00/105k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-026.parquet: 100%|████████| 102k/102k [00:00<00:00, 824kB/s]\u001b[A\n",
      "\n",
      "Fetching 154 files:  19%|████▏                 | 29/154 [00:01<00:03, 36.75it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "data/chunk-000/file-028.parquet: 100%|████████| 103k/103k [00:00<00:00, 698kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "data/chunk-000/file-029.parquet: 100%|███████| 105k/105k [00:00<00:00, 1.02MB/s]\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.2M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.2M/46.2M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.4M/46.4M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  42%|█████████▎            | 65/154 [00:02<00:02, 38.44it/s]\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.7M/47.7M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.8M/47.8M [00:01<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.9M/46.9M [00:01<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.2M/47.2M [00:01<00:00, 3\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.3M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.5M/46.5M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.7M/47.7M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.1M/47.1M [00:01<00:00, 2\u001b[A\n",
      "Fetching 154 files:  46%|██████████▏           | 71/154 [00:03<00:03, 21.75it/s]\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.3M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.0M/47.0M [00:01<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.2M/47.2M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.0M/47.0M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.6M/46.6M [00:01<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  49%|██████████▋           | 75/154 [00:03<00:04, 16.02it/s]\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.3M/47.3M [00:01<00:00, 4\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.3M/47.3M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.8M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.0M/47.0M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  52%|███████████▍          | 80/154 [00:04<00:04, 16.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.4M/47.4M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.2M/47.2M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.5M/46.5M [00:00<00:00, 7\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/46.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.8M/46.8M [00:00<00:00, 5\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.1M/47.1M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.5M/47.5M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.3M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…):   0%|  | 0.00/47.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.3M/47.3M [00:01<00:00, 2\u001b[A\n",
      "Fetching 154 files:  54%|███████████▊          | 83/154 [00:04<00:06, 10.69it/s]\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.2M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.4M/47.4M [00:01<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  57%|████████████▌         | 88/154 [00:05<00:05, 12.54it/s]\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.1M/46.1M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/32.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 46.5M/46.5M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/29.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.5M/47.5M [00:00<00:00, 6\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.7M/47.7M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  59%|█████████████         | 91/154 [00:05<00:05, 10.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.2M/47.2M [00:00<00:00, 6\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/30.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/26.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.2M/28.2M [00:00<00:00, 4\u001b[A\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/29.9M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "videos/observation.images.camera1/chunk-(…): 100%|█| 47.3M/47.3M [00:00<00:00, 5\u001b[A\u001b[A\n",
      "Fetching 154 files:  60%|█████████████▎        | 93/154 [00:05<00:05, 11.46it/s]\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/29.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.3M/28.3M [00:00<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 32.0M/32.0M [00:00<00:00, 3\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  62%|█████████████▋        | 96/154 [00:06<00:05, 10.51it/s]\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/27.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/31.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 26.8M/26.8M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 30.2M/30.2M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 30.2M/30.2M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 29.9M/29.9M [00:00<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 29.2M/29.2M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.7M/28.7M [00:00<00:00, 4\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.7M/28.7M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.1M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/30.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 29.9M/29.9M [00:00<00:00, 3\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 29.9M/29.9M [00:00<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  66%|█████████████▊       | 101/154 [00:06<00:04, 11.25it/s]\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/27.7M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 27.6M/27.6M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 30.3M/30.3M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 26.3M/26.3M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 27.7M/27.7M [00:00<00:00, 2\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/29.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 27.7M/27.7M [00:00<00:00, 4\u001b[A\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.3M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/27.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 31.4M/31.4M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  68%|██████████████▏      | 104/154 [00:07<00:06,  7.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/27.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.5M/28.5M [00:01<00:00, 2\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  69%|██████████████▍      | 106/154 [00:07<00:05,  8.47it/s]\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.1M/28.1M [00:00<00:00, 2\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/26.2M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 29.0M/29.0M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 29.0M/29.0M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.9M/28.9M [00:00<00:00, 4\n",
      "Fetching 154 files:  73%|███████████████▎     | 112/154 [00:07<00:03, 10.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.9M/28.9M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/25.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/26.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 27.7M/27.7M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.3M/28.3M [00:00<00:00, 3\u001b[A\n",
      "Fetching 154 files:  74%|███████████████▌     | 114/154 [00:07<00:03, 11.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 27.3M/27.3M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/28.0M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…):   0%|  | 0.00/29.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 26.2M/26.2M [00:00<00:00, 4\u001b[A\u001b[A\n",
      "Fetching 154 files:  77%|████████████████     | 118/154 [00:08<00:02, 14.28it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.1M/28.1M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.0M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/43.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.3M/28.3M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 25.8M/25.8M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  78%|████████████████▎    | 120/154 [00:08<00:03, 11.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 26.6M/26.6M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 28.0M/28.0M [00:00<00:00, 4\u001b[A\n",
      "Fetching 154 files:  80%|████████████████▊    | 123/154 [00:08<00:02, 10.96it/s]\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.6M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera2/chunk-(…): 100%|█| 29.7M/29.7M [00:00<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 43.5M/43.5M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.0M/45.0M [00:01<00:00, 4\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.2M/45.2M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/43.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.3M/45.3M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.6M/44.6M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.6M/45.6M [00:01<00:00, 4\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.6M/45.6M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.8M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.1M/45.1M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.8M/44.8M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  81%|█████████████████    | 125/154 [00:10<00:06,  4.19it/s]\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.6M/45.6M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/43.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 43.7M/43.7M [00:01<00:00, 4\u001b[A\u001b[A\n",
      "Fetching 154 files:  87%|██████████████████▎  | 134/154 [00:10<00:02,  9.34it/s]\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.0M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.6M/44.6M [00:01<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.8M/45.8M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.5M/45.5M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.7M/44.7M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.4M/44.4M [00:00<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/46.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.0M/45.0M [00:01<00:00, 4\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.8M/44.8M [00:01<00:00, 2\u001b[A\n",
      "Fetching 154 files:  89%|██████████████████▋  | 137/154 [00:11<00:02,  6.06it/s]\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/46.2M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/44.9M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.7M/45.7M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.6M/44.6M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/46.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 43.3M/43.3M [00:01<00:00, 2\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  92%|███████████████████▎ | 142/154 [00:12<00:01,  6.68it/s]\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.7M/45.7M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.2M/45.2M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/46.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…):   0%|  | 0.00/45.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 44.9M/44.9M [00:00<00:00, 5\u001b[A\u001b[A\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 46.2M/46.2M [00:00<00:00, 5\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 46.2M/46.2M [00:01<00:00, 3\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  95%|████████████████████ | 147/154 [00:12<00:00,  7.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.2M/45.2M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files:  98%|████████████████████▌| 151/154 [00:12<00:00,  8.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 46.0M/46.0M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 45.4M/45.4M [00:00<00:00, 5\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "videos/observation.images.camera3/chunk-(…): 100%|█| 46.2M/46.2M [00:01<00:00, 4\u001b[A\u001b[A\u001b[A\n",
      "Fetching 154 files: 100%|█████████████████████| 154/154 [00:13<00:00, 11.63it/s]\n",
      "INFO 2025-12-14 03:52:19 ot_train.py:202 Creating policy\n",
      "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Reducing the number of VLM layers to 16 ...\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:247 Creating optimizer and scheduler\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:259 \u001b[1m\u001b[33mOutput dir:\u001b[0m outputs/train/mission2_smolvla_multitask_30ksteps_120ep\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:262 cfg.steps=30000 (30K)\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:263 dataset.num_frames=79878 (80K)\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:264 dataset.num_episodes=120\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:267 Effective batch size: 64 x 1 = 64\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:268 num_learnable_params=99880992 (100M)\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:269 num_total_params=450046176 (450M)\n",
      "INFO 2025-12-14 03:52:34 ot_train.py:324 Start offline training on a fixed dataset\n",
      "INFO 2025-12-14 03:55:28 ot_train.py:351 step:200 smpl:13K ep:19 epch:0.16 loss:0.063 grdn:0.542 lr:1.0e-05 updt_s:0.799 data_s:0.069\n",
      "WARNING 2025-12-14 03:55:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 03:55:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 03:57:49 ot_train.py:351 step:400 smpl:26K ep:38 epch:0.32 loss:0.035 grdn:0.398 lr:3.0e-05 updt_s:0.653 data_s:0.048\n",
      "WARNING 2025-12-14 03:57:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 03:57:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:00:05 ot_train.py:351 step:600 smpl:38K ep:58 epch:0.48 loss:0.030 grdn:0.395 lr:5.0e-05 updt_s:0.636 data_s:0.044\n",
      "WARNING 2025-12-14 04:00:05 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:00:05 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:02:20 ot_train.py:351 step:800 smpl:51K ep:77 epch:0.64 loss:0.030 grdn:0.418 lr:7.0e-05 updt_s:0.631 data_s:0.041\n",
      "WARNING 2025-12-14 04:02:20 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:02:20 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:04:38 ot_train.py:351 step:1K smpl:64K ep:96 epch:0.80 loss:0.031 grdn:0.433 lr:9.0e-05 updt_s:0.644 data_s:0.043\n",
      "WARNING 2025-12-14 04:04:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:04:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:06:57 ot_train.py:351 step:1K smpl:77K ep:115 epch:0.96 loss:0.030 grdn:0.398 lr:1.0e-04 updt_s:0.652 data_s:0.044\n",
      "WARNING 2025-12-14 04:06:57 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:06:57 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 04:09:25 ot_train.py:351 step:1K smpl:90K ep:135 epch:1.12 loss:0.029 grdn:0.381 lr:1.0e-04 updt_s:0.660 data_s:0.075\n",
      "WARNING 2025-12-14 04:09:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:09:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:11:44 ot_train.py:351 step:2K smpl:102K ep:154 epch:1.28 loss:0.028 grdn:0.370 lr:9.9e-05 updt_s:0.650 data_s:0.044\n",
      "WARNING 2025-12-14 04:11:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:11:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:14:04 ot_train.py:351 step:2K smpl:115K ep:173 epch:1.44 loss:0.027 grdn:0.349 lr:9.9e-05 updt_s:0.654 data_s:0.046\n",
      "WARNING 2025-12-14 04:14:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:14:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:16:24 ot_train.py:351 step:2K smpl:128K ep:192 epch:1.60 loss:0.025 grdn:0.320 lr:9.9e-05 updt_s:0.648 data_s:0.045\n",
      "WARNING 2025-12-14 04:16:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:16:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:18:43 ot_train.py:351 step:2K smpl:141K ep:212 epch:1.76 loss:0.026 grdn:0.322 lr:9.9e-05 updt_s:0.647 data_s:0.048\n",
      "WARNING 2025-12-14 04:18:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:18:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:21:03 ot_train.py:351 step:2K smpl:154K ep:231 epch:1.92 loss:0.025 grdn:0.308 lr:9.9e-05 updt_s:0.654 data_s:0.046\n",
      "WARNING 2025-12-14 04:21:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:21:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 04:23:29 ot_train.py:351 step:3K smpl:166K ep:250 epch:2.08 loss:0.025 grdn:0.295 lr:9.8e-05 updt_s:0.648 data_s:0.079\n",
      "WARNING 2025-12-14 04:23:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:23:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:25:49 ot_train.py:351 step:3K smpl:179K ep:269 epch:2.24 loss:0.025 grdn:0.288 lr:9.8e-05 updt_s:0.653 data_s:0.044\n",
      "WARNING 2025-12-14 04:25:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:25:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:28:08 ot_train.py:351 step:3K smpl:192K ep:288 epch:2.40 loss:0.024 grdn:0.287 lr:9.8e-05 updt_s:0.652 data_s:0.043\n",
      "WARNING 2025-12-14 04:28:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:28:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:30:28 ot_train.py:351 step:3K smpl:205K ep:308 epch:2.56 loss:0.023 grdn:0.270 lr:9.7e-05 updt_s:0.653 data_s:0.044\n",
      "WARNING 2025-12-14 04:30:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:30:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:32:48 ot_train.py:351 step:3K smpl:218K ep:327 epch:2.72 loss:0.023 grdn:0.267 lr:9.7e-05 updt_s:0.655 data_s:0.042\n",
      "WARNING 2025-12-14 04:32:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:32:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:35:09 ot_train.py:351 step:4K smpl:230K ep:346 epch:2.88 loss:0.022 grdn:0.266 lr:9.7e-05 updt_s:0.656 data_s:0.046\n",
      "WARNING 2025-12-14 04:35:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:35:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 04:37:37 ot_train.py:351 step:4K smpl:243K ep:365 epch:3.04 loss:0.022 grdn:0.264 lr:9.6e-05 updt_s:0.658 data_s:0.079\n",
      "WARNING 2025-12-14 04:37:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:37:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:39:58 ot_train.py:351 step:4K smpl:256K ep:385 epch:3.20 loss:0.022 grdn:0.258 lr:9.6e-05 updt_s:0.656 data_s:0.046\n",
      "WARNING 2025-12-14 04:39:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:39:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:42:19 ot_train.py:351 step:4K smpl:269K ep:404 epch:3.37 loss:0.022 grdn:0.248 lr:9.6e-05 updt_s:0.661 data_s:0.044\n",
      "WARNING 2025-12-14 04:42:19 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:42:19 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:44:41 ot_train.py:351 step:4K smpl:282K ep:423 epch:3.53 loss:0.021 grdn:0.249 lr:9.5e-05 updt_s:0.661 data_s:0.045\n",
      "WARNING 2025-12-14 04:44:41 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:44:41 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:47:02 ot_train.py:351 step:5K smpl:294K ep:442 epch:3.69 loss:0.022 grdn:0.256 lr:9.5e-05 updt_s:0.656 data_s:0.045\n",
      "WARNING 2025-12-14 04:47:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:47:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:49:23 ot_train.py:351 step:5K smpl:307K ep:462 epch:3.85 loss:0.021 grdn:0.243 lr:9.4e-05 updt_s:0.658 data_s:0.045\n",
      "WARNING 2025-12-14 04:49:23 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:49:23 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 04:51:50 ot_train.py:351 step:5K smpl:320K ep:481 epch:4.01 loss:0.021 grdn:0.253 lr:9.4e-05 updt_s:0.651 data_s:0.084\n",
      "WARNING 2025-12-14 04:51:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:51:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:51:50 ot_train.py:361 Checkpoint policy after step 5000\n",
      "INFO 2025-12-14 04:54:16 ot_train.py:351 step:5K smpl:333K ep:500 epch:4.17 loss:0.020 grdn:0.245 lr:9.3e-05 updt_s:0.663 data_s:0.042\n",
      "WARNING 2025-12-14 04:54:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:54:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:56:37 ot_train.py:351 step:5K smpl:346K ep:519 epch:4.33 loss:0.020 grdn:0.232 lr:9.3e-05 updt_s:0.656 data_s:0.044\n",
      "WARNING 2025-12-14 04:56:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:56:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 04:58:57 ot_train.py:351 step:6K smpl:358K ep:538 epch:4.49 loss:0.020 grdn:0.229 lr:9.2e-05 updt_s:0.653 data_s:0.044\n",
      "WARNING 2025-12-14 04:58:57 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 04:58:57 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:01:18 ot_train.py:351 step:6K smpl:371K ep:558 epch:4.65 loss:0.019 grdn:0.211 lr:9.2e-05 updt_s:0.656 data_s:0.045\n",
      "WARNING 2025-12-14 05:01:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:01:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:03:38 ot_train.py:351 step:6K smpl:384K ep:577 epch:4.81 loss:0.019 grdn:0.232 lr:9.1e-05 updt_s:0.654 data_s:0.044\n",
      "WARNING 2025-12-14 05:03:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:03:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:05:57 ot_train.py:351 step:6K smpl:397K ep:596 epch:4.97 loss:0.020 grdn:0.228 lr:9.0e-05 updt_s:0.654 data_s:0.044\n",
      "WARNING 2025-12-14 05:05:57 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:05:57 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 05:08:24 ot_train.py:351 step:6K smpl:410K ep:615 epch:5.13 loss:0.018 grdn:0.213 lr:9.0e-05 updt_s:0.649 data_s:0.082\n",
      "WARNING 2025-12-14 05:08:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:08:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:10:45 ot_train.py:351 step:7K smpl:422K ep:635 epch:5.29 loss:0.019 grdn:0.221 lr:8.9e-05 updt_s:0.660 data_s:0.042\n",
      "WARNING 2025-12-14 05:10:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:10:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:13:05 ot_train.py:351 step:7K smpl:435K ep:654 epch:5.45 loss:0.018 grdn:0.214 lr:8.8e-05 updt_s:0.656 data_s:0.041\n",
      "WARNING 2025-12-14 05:13:05 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:13:05 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:15:27 ot_train.py:351 step:7K smpl:448K ep:673 epch:5.61 loss:0.018 grdn:0.219 lr:8.8e-05 updt_s:0.669 data_s:0.041\n",
      "WARNING 2025-12-14 05:15:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:15:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:17:50 ot_train.py:351 step:7K smpl:461K ep:692 epch:5.77 loss:0.018 grdn:0.218 lr:8.7e-05 updt_s:0.669 data_s:0.041\n",
      "WARNING 2025-12-14 05:17:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:17:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:20:09 ot_train.py:351 step:7K smpl:474K ep:711 epch:5.93 loss:0.018 grdn:0.206 lr:8.6e-05 updt_s:0.655 data_s:0.041\n",
      "WARNING 2025-12-14 05:20:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:20:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 05:22:36 ot_train.py:351 step:8K smpl:486K ep:731 epch:6.09 loss:0.017 grdn:0.212 lr:8.6e-05 updt_s:0.649 data_s:0.082\n",
      "WARNING 2025-12-14 05:22:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:22:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:24:56 ot_train.py:351 step:8K smpl:499K ep:750 epch:6.25 loss:0.017 grdn:0.209 lr:8.5e-05 updt_s:0.655 data_s:0.044\n",
      "WARNING 2025-12-14 05:24:56 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:24:56 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:27:13 ot_train.py:351 step:8K smpl:512K ep:769 epch:6.41 loss:0.017 grdn:0.202 lr:8.4e-05 updt_s:0.639 data_s:0.044\n",
      "WARNING 2025-12-14 05:27:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:27:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:29:31 ot_train.py:351 step:8K smpl:525K ep:788 epch:6.57 loss:0.017 grdn:0.199 lr:8.3e-05 updt_s:0.640 data_s:0.044\n",
      "WARNING 2025-12-14 05:29:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:29:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:31:48 ot_train.py:351 step:8K smpl:538K ep:808 epch:6.73 loss:0.016 grdn:0.195 lr:8.3e-05 updt_s:0.644 data_s:0.043\n",
      "WARNING 2025-12-14 05:31:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:31:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:34:06 ot_train.py:351 step:9K smpl:550K ep:827 epch:6.89 loss:0.017 grdn:0.198 lr:8.2e-05 updt_s:0.644 data_s:0.045\n",
      "WARNING 2025-12-14 05:34:06 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:34:06 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 05:36:32 ot_train.py:351 step:9K smpl:563K ep:846 epch:7.05 loss:0.016 grdn:0.192 lr:8.1e-05 updt_s:0.641 data_s:0.084\n",
      "WARNING 2025-12-14 05:36:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:36:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:38:50 ot_train.py:351 step:9K smpl:576K ep:865 epch:7.21 loss:0.016 grdn:0.194 lr:8.0e-05 updt_s:0.643 data_s:0.043\n",
      "WARNING 2025-12-14 05:38:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:38:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:41:08 ot_train.py:351 step:9K smpl:589K ep:885 epch:7.37 loss:0.016 grdn:0.191 lr:7.9e-05 updt_s:0.647 data_s:0.044\n",
      "WARNING 2025-12-14 05:41:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:41:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:43:27 ot_train.py:351 step:9K smpl:602K ep:904 epch:7.53 loss:0.016 grdn:0.190 lr:7.9e-05 updt_s:0.646 data_s:0.043\n",
      "WARNING 2025-12-14 05:43:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:43:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:45:45 ot_train.py:351 step:10K smpl:614K ep:923 epch:7.69 loss:0.015 grdn:0.184 lr:7.8e-05 updt_s:0.644 data_s:0.044\n",
      "WARNING 2025-12-14 05:45:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:45:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:48:05 ot_train.py:351 step:10K smpl:627K ep:942 epch:7.85 loss:0.015 grdn:0.191 lr:7.7e-05 updt_s:0.653 data_s:0.043\n",
      "WARNING 2025-12-14 05:48:05 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:48:05 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 05:50:32 ot_train.py:351 step:10K smpl:640K ep:961 epch:8.01 loss:0.015 grdn:0.181 lr:7.6e-05 updt_s:0.646 data_s:0.089\n",
      "WARNING 2025-12-14 05:50:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:50:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:50:32 ot_train.py:361 Checkpoint policy after step 10000\n",
      "INFO 2025-12-14 05:52:55 ot_train.py:351 step:10K smpl:653K ep:981 epch:8.17 loss:0.015 grdn:0.181 lr:7.5e-05 updt_s:0.650 data_s:0.042\n",
      "WARNING 2025-12-14 05:52:55 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:52:55 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:55:15 ot_train.py:351 step:10K smpl:666K ep:1000 epch:8.33 loss:0.015 grdn:0.180 lr:7.4e-05 updt_s:0.650 data_s:0.044\n",
      "WARNING 2025-12-14 05:55:15 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:55:15 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:57:35 ot_train.py:351 step:11K smpl:678K ep:1K epch:8.49 loss:0.015 grdn:0.184 lr:7.3e-05 updt_s:0.657 data_s:0.045\n",
      "WARNING 2025-12-14 05:57:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:57:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 05:59:53 ot_train.py:351 step:11K smpl:691K ep:1K epch:8.65 loss:0.015 grdn:0.188 lr:7.2e-05 updt_s:0.639 data_s:0.045\n",
      "WARNING 2025-12-14 05:59:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 05:59:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:02:09 ot_train.py:351 step:11K smpl:704K ep:1K epch:8.81 loss:0.014 grdn:0.182 lr:7.2e-05 updt_s:0.633 data_s:0.044\n",
      "WARNING 2025-12-14 06:02:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:02:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:04:24 ot_train.py:351 step:11K smpl:717K ep:1K epch:8.97 loss:0.014 grdn:0.182 lr:7.1e-05 updt_s:0.632 data_s:0.042\n",
      "WARNING 2025-12-14 06:04:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:04:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 06:06:48 ot_train.py:351 step:11K smpl:730K ep:1K epch:9.13 loss:0.014 grdn:0.180 lr:7.0e-05 updt_s:0.631 data_s:0.086\n",
      "WARNING 2025-12-14 06:06:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:06:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:09:04 ot_train.py:351 step:12K smpl:742K ep:1K epch:9.29 loss:0.014 grdn:0.172 lr:6.9e-05 updt_s:0.635 data_s:0.046\n",
      "WARNING 2025-12-14 06:09:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:09:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:11:21 ot_train.py:351 step:12K smpl:755K ep:1K epch:9.45 loss:0.013 grdn:0.169 lr:6.8e-05 updt_s:0.636 data_s:0.045\n",
      "WARNING 2025-12-14 06:11:21 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:11:21 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:13:36 ot_train.py:351 step:12K smpl:768K ep:1K epch:9.61 loss:0.014 grdn:0.181 lr:6.7e-05 updt_s:0.626 data_s:0.046\n",
      "WARNING 2025-12-14 06:13:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:13:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:15:50 ot_train.py:351 step:12K smpl:781K ep:1K epch:9.77 loss:0.013 grdn:0.167 lr:6.6e-05 updt_s:0.627 data_s:0.044\n",
      "WARNING 2025-12-14 06:15:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:15:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:18:02 ot_train.py:351 step:12K smpl:794K ep:1K epch:9.94 loss:0.013 grdn:0.163 lr:6.5e-05 updt_s:0.616 data_s:0.043\n",
      "WARNING 2025-12-14 06:18:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:18:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 06:20:23 ot_train.py:351 step:13K smpl:806K ep:1K epch:10.10 loss:0.013 grdn:0.171 lr:6.4e-05 updt_s:0.615 data_s:0.086\n",
      "WARNING 2025-12-14 06:20:23 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:20:23 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:22:37 ot_train.py:351 step:13K smpl:819K ep:1K epch:10.26 loss:0.013 grdn:0.179 lr:6.3e-05 updt_s:0.624 data_s:0.045\n",
      "WARNING 2025-12-14 06:22:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:22:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:24:55 ot_train.py:351 step:13K smpl:832K ep:1K epch:10.42 loss:0.013 grdn:0.167 lr:6.2e-05 updt_s:0.645 data_s:0.044\n",
      "WARNING 2025-12-14 06:24:55 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:24:55 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:27:14 ot_train.py:351 step:13K smpl:845K ep:1K epch:10.58 loss:0.013 grdn:0.166 lr:6.1e-05 updt_s:0.645 data_s:0.045\n",
      "WARNING 2025-12-14 06:27:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:27:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:29:28 ot_train.py:351 step:13K smpl:858K ep:1K epch:10.74 loss:0.012 grdn:0.163 lr:6.0e-05 updt_s:0.620 data_s:0.047\n",
      "WARNING 2025-12-14 06:29:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:29:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:31:42 ot_train.py:351 step:14K smpl:870K ep:1K epch:10.90 loss:0.012 grdn:0.159 lr:5.9e-05 updt_s:0.623 data_s:0.044\n",
      "WARNING 2025-12-14 06:31:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:31:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 06:34:04 ot_train.py:351 step:14K smpl:883K ep:1K epch:11.06 loss:0.012 grdn:0.160 lr:5.8e-05 updt_s:0.621 data_s:0.087\n",
      "WARNING 2025-12-14 06:34:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:34:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:36:18 ot_train.py:351 step:14K smpl:896K ep:1K epch:11.22 loss:0.012 grdn:0.166 lr:5.7e-05 updt_s:0.624 data_s:0.043\n",
      "WARNING 2025-12-14 06:36:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:36:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:38:33 ot_train.py:351 step:14K smpl:909K ep:1K epch:11.38 loss:0.012 grdn:0.168 lr:5.6e-05 updt_s:0.627 data_s:0.043\n",
      "WARNING 2025-12-14 06:38:33 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:38:33 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:40:47 ot_train.py:351 step:14K smpl:922K ep:1K epch:11.54 loss:0.012 grdn:0.160 lr:5.5e-05 updt_s:0.625 data_s:0.045\n",
      "WARNING 2025-12-14 06:40:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:40:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:43:03 ot_train.py:351 step:15K smpl:934K ep:1K epch:11.70 loss:0.012 grdn:0.160 lr:5.4e-05 updt_s:0.629 data_s:0.047\n",
      "WARNING 2025-12-14 06:43:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:43:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:45:18 ot_train.py:351 step:15K smpl:947K ep:1K epch:11.86 loss:0.012 grdn:0.158 lr:5.3e-05 updt_s:0.627 data_s:0.046\n",
      "WARNING 2025-12-14 06:45:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:45:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 06:47:40 ot_train.py:351 step:15K smpl:960K ep:1K epch:12.02 loss:0.012 grdn:0.162 lr:5.2e-05 updt_s:0.623 data_s:0.084\n",
      "WARNING 2025-12-14 06:47:40 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:47:40 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:47:40 ot_train.py:361 Checkpoint policy after step 15000\n",
      "INFO 2025-12-14 06:49:59 ot_train.py:351 step:15K smpl:973K ep:1K epch:12.18 loss:0.011 grdn:0.154 lr:5.1e-05 updt_s:0.624 data_s:0.045\n",
      "WARNING 2025-12-14 06:49:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:49:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:52:14 ot_train.py:351 step:15K smpl:986K ep:1K epch:12.34 loss:0.011 grdn:0.149 lr:5.0e-05 updt_s:0.631 data_s:0.044\n",
      "WARNING 2025-12-14 06:52:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:52:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:54:30 ot_train.py:351 step:16K smpl:998K ep:1K epch:12.50 loss:0.011 grdn:0.148 lr:4.9e-05 updt_s:0.631 data_s:0.044\n",
      "WARNING 2025-12-14 06:54:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:54:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:56:46 ot_train.py:351 step:16K smpl:1M ep:2K epch:12.66 loss:0.011 grdn:0.146 lr:4.8e-05 updt_s:0.636 data_s:0.042\n",
      "WARNING 2025-12-14 06:56:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:56:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 06:58:58 ot_train.py:351 step:16K smpl:1M ep:2K epch:12.82 loss:0.011 grdn:0.151 lr:4.7e-05 updt_s:0.616 data_s:0.042\n",
      "WARNING 2025-12-14 06:58:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 06:58:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:01:10 ot_train.py:351 step:16K smpl:1M ep:2K epch:12.98 loss:0.011 grdn:0.146 lr:4.6e-05 updt_s:0.621 data_s:0.040\n",
      "WARNING 2025-12-14 07:01:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:01:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 07:03:35 ot_train.py:351 step:16K smpl:1M ep:2K epch:13.14 loss:0.011 grdn:0.142 lr:4.5e-05 updt_s:0.626 data_s:0.096\n",
      "WARNING 2025-12-14 07:03:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:03:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:05:50 ot_train.py:351 step:17K smpl:1M ep:2K epch:13.30 loss:0.011 grdn:0.141 lr:4.4e-05 updt_s:0.629 data_s:0.043\n",
      "WARNING 2025-12-14 07:05:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:05:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:08:03 ot_train.py:351 step:17K smpl:1M ep:2K epch:13.46 loss:0.011 grdn:0.150 lr:4.3e-05 updt_s:0.616 data_s:0.042\n",
      "WARNING 2025-12-14 07:08:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:08:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:10:16 ot_train.py:351 step:17K smpl:1M ep:2K epch:13.62 loss:0.011 grdn:0.142 lr:4.2e-05 updt_s:0.619 data_s:0.046\n",
      "WARNING 2025-12-14 07:10:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:10:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:12:31 ot_train.py:351 step:17K smpl:1M ep:2K epch:13.78 loss:0.010 grdn:0.143 lr:4.1e-05 updt_s:0.625 data_s:0.047\n",
      "WARNING 2025-12-14 07:12:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:12:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:14:44 ot_train.py:351 step:17K smpl:1M ep:2K epch:13.94 loss:0.010 grdn:0.138 lr:4.0e-05 updt_s:0.621 data_s:0.045\n",
      "WARNING 2025-12-14 07:14:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:14:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 07:17:07 ot_train.py:351 step:18K smpl:1M ep:2K epch:14.10 loss:0.011 grdn:0.147 lr:3.9e-05 updt_s:0.618 data_s:0.092\n",
      "WARNING 2025-12-14 07:17:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:17:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:19:19 ot_train.py:351 step:18K smpl:1M ep:2K epch:14.26 loss:0.010 grdn:0.142 lr:3.8e-05 updt_s:0.618 data_s:0.043\n",
      "WARNING 2025-12-14 07:19:19 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:19:19 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:21:34 ot_train.py:351 step:18K smpl:1M ep:2K epch:14.42 loss:0.010 grdn:0.141 lr:3.7e-05 updt_s:0.624 data_s:0.045\n",
      "WARNING 2025-12-14 07:21:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:21:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:23:46 ot_train.py:351 step:18K smpl:1M ep:2K epch:14.58 loss:0.010 grdn:0.132 lr:3.6e-05 updt_s:0.618 data_s:0.044\n",
      "WARNING 2025-12-14 07:23:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:23:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:26:01 ot_train.py:351 step:18K smpl:1M ep:2K epch:14.74 loss:0.010 grdn:0.137 lr:3.5e-05 updt_s:0.624 data_s:0.044\n",
      "WARNING 2025-12-14 07:26:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:26:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:28:15 ot_train.py:351 step:19K smpl:1M ep:2K epch:14.90 loss:0.010 grdn:0.137 lr:3.4e-05 updt_s:0.625 data_s:0.046\n",
      "WARNING 2025-12-14 07:28:15 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:28:15 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 07:30:42 ot_train.py:351 step:19K smpl:1M ep:2K epch:15.06 loss:0.010 grdn:0.138 lr:3.3e-05 updt_s:0.629 data_s:0.103\n",
      "WARNING 2025-12-14 07:30:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:30:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:32:59 ot_train.py:351 step:19K smpl:1M ep:2K epch:15.22 loss:0.010 grdn:0.134 lr:3.2e-05 updt_s:0.637 data_s:0.047\n",
      "WARNING 2025-12-14 07:32:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:32:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:35:14 ot_train.py:351 step:19K smpl:1M ep:2K epch:15.38 loss:0.010 grdn:0.134 lr:3.1e-05 updt_s:0.626 data_s:0.045\n",
      "WARNING 2025-12-14 07:35:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:35:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:37:26 ot_train.py:351 step:19K smpl:1M ep:2K epch:15.54 loss:0.010 grdn:0.131 lr:3.0e-05 updt_s:0.619 data_s:0.042\n",
      "WARNING 2025-12-14 07:37:26 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:37:26 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:39:39 ot_train.py:351 step:20K smpl:1M ep:2K epch:15.70 loss:0.010 grdn:0.130 lr:2.9e-05 updt_s:0.616 data_s:0.046\n",
      "WARNING 2025-12-14 07:39:39 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:39:39 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:41:55 ot_train.py:351 step:20K smpl:1M ep:2K epch:15.86 loss:0.010 grdn:0.130 lr:2.8e-05 updt_s:0.632 data_s:0.044\n",
      "WARNING 2025-12-14 07:41:55 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:41:55 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 07:44:18 ot_train.py:351 step:20K smpl:1M ep:2K epch:16.02 loss:0.010 grdn:0.130 lr:2.7e-05 updt_s:0.623 data_s:0.090\n",
      "WARNING 2025-12-14 07:44:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:44:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:44:18 ot_train.py:361 Checkpoint policy after step 20000\n",
      "INFO 2025-12-14 07:46:36 ot_train.py:351 step:20K smpl:1M ep:2K epch:16.18 loss:0.009 grdn:0.130 lr:2.6e-05 updt_s:0.622 data_s:0.043\n",
      "WARNING 2025-12-14 07:46:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:46:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:48:50 ot_train.py:351 step:20K smpl:1M ep:2K epch:16.34 loss:0.009 grdn:0.125 lr:2.6e-05 updt_s:0.620 data_s:0.044\n",
      "WARNING 2025-12-14 07:48:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:48:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:51:03 ot_train.py:351 step:21K smpl:1M ep:2K epch:16.51 loss:0.009 grdn:0.128 lr:2.5e-05 updt_s:0.620 data_s:0.044\n",
      "WARNING 2025-12-14 07:51:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:51:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:53:20 ot_train.py:351 step:21K smpl:1M ep:2K epch:16.67 loss:0.009 grdn:0.124 lr:2.4e-05 updt_s:0.637 data_s:0.046\n",
      "WARNING 2025-12-14 07:53:20 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:53:20 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:55:41 ot_train.py:351 step:21K smpl:1M ep:2K epch:16.83 loss:0.009 grdn:0.126 lr:2.3e-05 updt_s:0.657 data_s:0.046\n",
      "WARNING 2025-12-14 07:55:41 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:55:41 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 07:58:03 ot_train.py:351 step:21K smpl:1M ep:2K epch:16.99 loss:0.009 grdn:0.121 lr:2.2e-05 updt_s:0.658 data_s:0.047\n",
      "WARNING 2025-12-14 07:58:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 07:58:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 08:00:31 ot_train.py:351 step:21K smpl:1M ep:2K epch:17.15 loss:0.009 grdn:0.119 lr:2.1e-05 updt_s:0.645 data_s:0.090\n",
      "WARNING 2025-12-14 08:00:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:00:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:02:51 ot_train.py:351 step:22K smpl:1M ep:2K epch:17.31 loss:0.009 grdn:0.121 lr:2.1e-05 updt_s:0.655 data_s:0.046\n",
      "WARNING 2025-12-14 08:02:51 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:02:51 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:05:11 ot_train.py:351 step:22K smpl:1M ep:2K epch:17.47 loss:0.009 grdn:0.121 lr:2.0e-05 updt_s:0.650 data_s:0.044\n",
      "WARNING 2025-12-14 08:05:11 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:05:11 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:07:27 ot_train.py:351 step:22K smpl:1M ep:2K epch:17.63 loss:0.009 grdn:0.122 lr:1.9e-05 updt_s:0.634 data_s:0.044\n",
      "WARNING 2025-12-14 08:07:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:07:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:09:39 ot_train.py:351 step:22K smpl:1M ep:2K epch:17.79 loss:0.009 grdn:0.124 lr:1.8e-05 updt_s:0.618 data_s:0.041\n",
      "WARNING 2025-12-14 08:09:39 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:09:39 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:11:52 ot_train.py:351 step:22K smpl:1M ep:2K epch:17.95 loss:0.009 grdn:0.121 lr:1.8e-05 updt_s:0.621 data_s:0.043\n",
      "WARNING 2025-12-14 08:11:52 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:11:52 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 08:14:13 ot_train.py:351 step:23K smpl:1M ep:2K epch:18.11 loss:0.009 grdn:0.121 lr:1.7e-05 updt_s:0.614 data_s:0.085\n",
      "WARNING 2025-12-14 08:14:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:14:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:16:27 ot_train.py:351 step:23K smpl:1M ep:2K epch:18.27 loss:0.009 grdn:0.120 lr:1.6e-05 updt_s:0.624 data_s:0.043\n",
      "WARNING 2025-12-14 08:16:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:16:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:18:42 ot_train.py:351 step:23K smpl:1M ep:2K epch:18.43 loss:0.009 grdn:0.116 lr:1.5e-05 updt_s:0.630 data_s:0.043\n",
      "WARNING 2025-12-14 08:18:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:18:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:20:53 ot_train.py:351 step:23K smpl:1M ep:2K epch:18.59 loss:0.009 grdn:0.129 lr:1.5e-05 updt_s:0.613 data_s:0.043\n",
      "WARNING 2025-12-14 08:20:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:20:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:23:03 ot_train.py:351 step:23K smpl:1M ep:2K epch:18.75 loss:0.009 grdn:0.114 lr:1.4e-05 updt_s:0.605 data_s:0.041\n",
      "WARNING 2025-12-14 08:23:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:23:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:25:13 ot_train.py:351 step:24K smpl:2M ep:2K epch:18.91 loss:0.009 grdn:0.119 lr:1.3e-05 updt_s:0.603 data_s:0.043\n",
      "WARNING 2025-12-14 08:25:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:25:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 08:27:34 ot_train.py:351 step:24K smpl:2M ep:2K epch:19.07 loss:0.009 grdn:0.112 lr:1.3e-05 updt_s:0.619 data_s:0.085\n",
      "WARNING 2025-12-14 08:27:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:27:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:29:44 ot_train.py:351 step:24K smpl:2M ep:2K epch:19.23 loss:0.009 grdn:0.116 lr:1.2e-05 updt_s:0.606 data_s:0.044\n",
      "WARNING 2025-12-14 08:29:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:29:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:31:56 ot_train.py:351 step:24K smpl:2M ep:2K epch:19.39 loss:0.008 grdn:0.110 lr:1.2e-05 updt_s:0.618 data_s:0.040\n",
      "WARNING 2025-12-14 08:31:56 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:31:56 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:34:07 ot_train.py:351 step:24K smpl:2M ep:2K epch:19.55 loss:0.009 grdn:0.113 lr:1.1e-05 updt_s:0.604 data_s:0.045\n",
      "WARNING 2025-12-14 08:34:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:34:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:36:19 ot_train.py:351 step:25K smpl:2M ep:2K epch:19.71 loss:0.009 grdn:0.112 lr:1.0e-05 updt_s:0.618 data_s:0.040\n",
      "WARNING 2025-12-14 08:36:19 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:36:19 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:38:31 ot_train.py:351 step:25K smpl:2M ep:2K epch:19.87 loss:0.009 grdn:0.112 lr:9.8e-06 updt_s:0.619 data_s:0.041\n",
      "WARNING 2025-12-14 08:38:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:38:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 08:40:49 ot_train.py:351 step:25K smpl:2M ep:2K epch:20.03 loss:0.009 grdn:0.113 lr:9.3e-06 updt_s:0.609 data_s:0.079\n",
      "WARNING 2025-12-14 08:40:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:40:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:40:49 ot_train.py:361 Checkpoint policy after step 25000\n",
      "INFO 2025-12-14 08:43:09 ot_train.py:351 step:25K smpl:2M ep:2K epch:20.19 loss:0.009 grdn:0.112 lr:8.8e-06 updt_s:0.631 data_s:0.043\n",
      "WARNING 2025-12-14 08:43:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:43:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:45:25 ot_train.py:351 step:25K smpl:2M ep:2K epch:20.35 loss:0.008 grdn:0.106 lr:8.3e-06 updt_s:0.634 data_s:0.044\n",
      "WARNING 2025-12-14 08:45:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:45:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:47:42 ot_train.py:351 step:26K smpl:2M ep:2K epch:20.51 loss:0.009 grdn:0.110 lr:7.8e-06 updt_s:0.637 data_s:0.046\n",
      "WARNING 2025-12-14 08:47:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:47:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:49:58 ot_train.py:351 step:26K smpl:2M ep:2K epch:20.67 loss:0.008 grdn:0.109 lr:7.4e-06 updt_s:0.633 data_s:0.045\n",
      "WARNING 2025-12-14 08:49:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:49:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:52:12 ot_train.py:351 step:26K smpl:2M ep:2K epch:20.83 loss:0.009 grdn:0.111 lr:6.9e-06 updt_s:0.624 data_s:0.045\n",
      "WARNING 2025-12-14 08:52:12 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:52:12 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:54:22 ot_train.py:351 step:26K smpl:2M ep:3K epch:20.99 loss:0.008 grdn:0.104 lr:6.5e-06 updt_s:0.605 data_s:0.041\n",
      "WARNING 2025-12-14 08:54:22 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:54:22 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 08:56:43 ot_train.py:351 step:26K smpl:2M ep:3K epch:21.15 loss:0.008 grdn:0.111 lr:6.1e-06 updt_s:0.609 data_s:0.095\n",
      "WARNING 2025-12-14 08:56:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:56:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 08:58:54 ot_train.py:351 step:27K smpl:2M ep:3K epch:21.31 loss:0.008 grdn:0.106 lr:5.7e-06 updt_s:0.613 data_s:0.039\n",
      "WARNING 2025-12-14 08:58:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 08:58:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:01:04 ot_train.py:351 step:27K smpl:2M ep:3K epch:21.47 loss:0.008 grdn:0.105 lr:5.4e-06 updt_s:0.609 data_s:0.040\n",
      "WARNING 2025-12-14 09:01:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:01:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:03:18 ot_train.py:351 step:27K smpl:2M ep:3K epch:21.63 loss:0.008 grdn:0.102 lr:5.0e-06 updt_s:0.626 data_s:0.044\n",
      "WARNING 2025-12-14 09:03:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:03:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:05:30 ot_train.py:351 step:27K smpl:2M ep:3K epch:21.79 loss:0.009 grdn:0.104 lr:4.7e-06 updt_s:0.614 data_s:0.042\n",
      "WARNING 2025-12-14 09:05:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:05:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:07:40 ot_train.py:351 step:27K smpl:2M ep:3K epch:21.95 loss:0.008 grdn:0.104 lr:4.4e-06 updt_s:0.610 data_s:0.041\n",
      "WARNING 2025-12-14 09:07:40 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:07:40 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 09:10:00 ot_train.py:351 step:28K smpl:2M ep:3K epch:22.11 loss:0.008 grdn:0.106 lr:4.2e-06 updt_s:0.612 data_s:0.083\n",
      "WARNING 2025-12-14 09:10:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:10:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:12:13 ot_train.py:351 step:28K smpl:2M ep:3K epch:22.27 loss:0.008 grdn:0.107 lr:3.9e-06 updt_s:0.622 data_s:0.044\n",
      "WARNING 2025-12-14 09:12:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:12:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:14:25 ot_train.py:351 step:28K smpl:2M ep:3K epch:22.43 loss:0.009 grdn:0.106 lr:3.7e-06 updt_s:0.613 data_s:0.043\n",
      "WARNING 2025-12-14 09:14:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:14:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:16:37 ot_train.py:351 step:28K smpl:2M ep:3K epch:22.59 loss:0.008 grdn:0.102 lr:3.5e-06 updt_s:0.617 data_s:0.042\n",
      "WARNING 2025-12-14 09:16:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:16:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:18:24 ot_train.py:351 step:28K smpl:2M ep:3K epch:22.75 loss:0.008 grdn:0.103 lr:3.3e-06 updt_s:0.511 data_s:0.023\n",
      "WARNING 2025-12-14 09:18:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:18:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:20:07 ot_train.py:351 step:29K smpl:2M ep:3K epch:22.91 loss:0.008 grdn:0.102 lr:3.1e-06 updt_s:0.489 data_s:0.020\n",
      "WARNING 2025-12-14 09:20:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:20:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 09:21:57 ot_train.py:351 step:29K smpl:2M ep:3K epch:23.08 loss:0.009 grdn:0.107 lr:3.0e-06 updt_s:0.490 data_s:0.062\n",
      "WARNING 2025-12-14 09:21:57 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:21:57 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:23:41 ot_train.py:351 step:29K smpl:2M ep:3K epch:23.24 loss:0.008 grdn:0.102 lr:2.8e-06 updt_s:0.495 data_s:0.020\n",
      "WARNING 2025-12-14 09:23:41 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:23:41 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:25:24 ot_train.py:351 step:29K smpl:2M ep:3K epch:23.40 loss:0.008 grdn:0.100 lr:2.7e-06 updt_s:0.494 data_s:0.020\n",
      "WARNING 2025-12-14 09:25:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:25:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:27:06 ot_train.py:351 step:29K smpl:2M ep:3K epch:23.56 loss:0.008 grdn:0.101 lr:2.6e-06 updt_s:0.491 data_s:0.020\n",
      "WARNING 2025-12-14 09:27:06 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:27:06 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:28:49 ot_train.py:351 step:30K smpl:2M ep:3K epch:23.72 loss:0.009 grdn:0.105 lr:2.6e-06 updt_s:0.494 data_s:0.020\n",
      "WARNING 2025-12-14 09:28:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:28:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:30:32 ot_train.py:351 step:30K smpl:2M ep:3K epch:23.88 loss:0.008 grdn:0.101 lr:2.5e-06 updt_s:0.493 data_s:0.020\n",
      "WARNING 2025-12-14 09:30:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:30:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-14 09:32:21 ot_train.py:351 step:30K smpl:2M ep:3K epch:24.04 loss:0.008 grdn:0.102 lr:2.5e-06 updt_s:0.486 data_s:0.054\n",
      "WARNING 2025-12-14 09:32:21 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-14 09:32:21 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-14 09:32:21 ot_train.py:361 Checkpoint policy after step 30000\n",
      "INFO 2025-12-14 09:32:25 ot_train.py:430 End of training\n",
      "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
      "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...s_120ep/model.safetensors:   0%|              | 3.67MB /  907MB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   0%|              | 3.67MB /  907MB, 9.18MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :   5%|▊             | 3.67MB / 67.1MB, 9.18MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   4%|▌             | 34.9MB /  907MB, 58.1MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  26%|███▋          | 34.9MB /  134MB, 58.1MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   8%|█▏            | 74.8MB /  907MB, 93.6MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  37%|█████▏        | 74.8MB /  201MB, 93.6MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  22%|███           |  196MB /  907MB,  196MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  54%|███████▌      |  109MB /  201MB,  109MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  38%|█████▎        |  342MB /  907MB,  285MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  77%|██████████▋   |  154MB /  201MB,  129MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  55%|███████▋      |  496MB /  907MB,  355MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               :  95%|█████████████▎|  191MB /  201MB,  137MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  68%|█████████▍    |  615MB /  907MB,  384MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉|  200MB /  201MB,  125MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  80%|███████████▏  |  724MB /  907MB,  402MB/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  93%|████████████▉ |  842MB /  907MB,  421MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉|  201MB /  201MB,  101MB/s  \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100%|█████████████▉|  906MB /  907MB,  412MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|█████████████▉|  206MB /  207MB, 93.7MB/s  \u001b[A\n",
      "\n",
      "Processing Files (1 / 1)      : 100%|██████████████|  907MB /  907MB,  378MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|██████████████|  207MB /  207MB, 86.1MB/s  \u001b[A\n",
      "\n",
      "  ...s_120ep/model.safetensors: 100%|██████████████|  907MB /  907MB            \u001b[A\u001b[A\n",
      "\n",
      "  ...s_120ep/model.safetensors: 100%|██████████████|  907MB /  907MB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (1 / 1)      : 100%|██████████████|  907MB /  907MB,  324MB/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|██████████████|  207MB /  207MB, 73.8MB/s  \n",
      "  ...s_120ep/model.safetensors: 100%|██████████████|  907MB /  907MB            \n",
      "INFO 2025-12-14 09:32:32 etrained.py:237 Model pushed to https://huggingface.co/jlamperez/mission2_smolvla_multitask_policy_30ksteps_120ep\n",
      "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
      "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 8.61kB / 8.61kB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (1 / 1)      : 100%|██████████████| 8.61kB / 8.61kB,   ???B/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|██████████████| 8.61kB / 8.61kB,   ???B/s  \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 8.61kB / 8.61kB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (1 / 1)      : 100%|██████████████| 8.61kB / 8.61kB,  0.00B/s  \u001b[A\u001b[A\n",
      "New Data Upload               : 100%|██████████████| 8.61kB / 8.61kB,  0.00B/s  \n",
      "  ...zer_processor.safetensors: 100%|██████████████| 8.61kB / 8.61kB            \n",
      "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
      "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 8.61kB / 8.61kB            \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (1 / 1)      : 100%|██████████████| 8.61kB / 8.61kB,   ???B/s  \u001b[A\u001b[A\n",
      "\n",
      "Processing Files (1 / 1)      : 100%|██████████████| 8.61kB / 8.61kB,  0.00B/s  \u001b[A\u001b[A\n",
      "New Data Upload               : |                  |  0.00B /  0.00B,  0.00B/s  \n",
      "  ...zer_processor.safetensors: 100%|██████████████| 8.61kB / 8.61kB            \n"
     ]
    }
   ],
   "source": [
    "!lerobot-train \\\n",
    "  --policy.path=lerobot/smolvla_base \\\n",
    "  --dataset.repo_id=\"jlamperez/mission2_smolvla_multitask_v2_120ep\" \\\n",
    "  --policy.repo_id=\"jlamperez/mission2_smolvla_multitask_policy_30ksteps_120ep\" \\\n",
    "  --batch_size=64 \\\n",
    "  --steps=30000 \\\n",
    "  --save_freq=5000 \\\n",
    "  --output_dir=\"outputs/train/mission2_smolvla_multitask_30ksteps_120ep\" \\\n",
    "  --job_name=\"my_smolvla_training_30ksteps_120ep\" \\\n",
    "  --policy.device=cuda \\\n",
    "  --wandb.enable=true \\\n",
    "  --policy.use_amp=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "- If using a local dataset, add `--dataset.root=/path/to/dataset`.\n",
    "- Adjust `--batch_size` and `--steps` based on your hardware and dataset.\n",
    "- Model checkpoints, logs, and training plots will be saved to the specified `--output_dir`\n",
    "- Training progress visualized in your wandb dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models from Hugging Face to Local Machine\n",
    "Now after training is done, download the model to local machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFMLGuVkH7UN",
    "outputId": "535f0717-4d6b-4eeb-beee-25416f7af383"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download ${HF_USER}/{REPO_NAME} --repo-type model --local-dir path/to/model\n",
    "# e.g. huggingface-cli upload ${HF_USER}/act_so101_3cube_1ksteps \\\n",
    "#  outputs/train/act_so101_3cube_1ksteps/checkpoints/last/pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscs\n",
    "1. Once the environment is setup, you can open a terminal session for training by navigating to `File → New Launcher → Other → Terminal`.\n",
    "2. You can also upload your datasets to the container by clicking the `Upload Files` button in the left pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "1. If you encounter an error like:\n",
    "   ```\n",
    "   FileExistsError: Output directory outputs/train/act_so101_3cube_1ksteps already exists and resume is False. Please change your output directory so that outputs/train/act_so101_3cube_1ksteps is not overwritten. \n",
    "   ```\n",
    "   Remove the existing directory before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFMLGuVkH7UN",
    "outputId": "535f0717-4d6b-4eeb-beee-25416f7af383"
   },
   "outputs": [],
   "source": [
    "!rm -fr outputs/train/act_so101_3cube_1ksteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When running models other than ACT, ensure you install the required additional dependencies for those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For smolVLA\n",
    "!cd lerobot && pip install -e \".[smolvla]\"\n",
    "# For Pi\n",
    "!cd lerobot && pip install -e \".[pi]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you want to resume the training from last checkpoint, run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lerobot-train \\\n",
    "  --resume=true \\\n",
    "  --config_path=outputs/train/<job name>/checkpoints/last/pretrained_model/train_config.json \\\n",
    "  --steps=<new total steps>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If you want to upload your dataset using `huggingface-cli upload <repo name> <path to the dataset> --repo-type=dataset`, be sure to set a codebase tag like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"your_huggingface_token\")\n",
    "hub_api = HfApi()\n",
    "hub_api.create_tag(<HF_REPO_NAME>, tag=\"v3.0\", revision=\"main\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/venv/bin/python: No module named uv\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "mkdir -p outputs/train/act_so101_mission1_pick_place\n",
    "uv run huggingface-cli download jlamperez/so101_act_mission1_pick_place --repo-type model --local-dir outputs/train/act_so101_mission1_pick_place/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
